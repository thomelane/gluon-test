{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. Transfer Learning with Your Own Image Dataset\n=======================================================\n\nDataset size is a big factor in the performance of deep learning models.\n``ImageNet`` has over one million labeled images, but\nwe often don't have so much labeled data in other domains.\nTraining a deep learning models on small datasets may lead to severe overfitting.\n\nTransfer learning is a technique that addresses this problem.\nThe idea is simple: we can start training with a pre-trained model,\ninstead of starting from scratch.\nAs Isaac Newton said, \"If I have seen further it is by standing on the\nshoulders of Giants\".\n\nIn this tutorial, we will explain the basics of transfer\nlearning, and apply it to the ``MINC-2500`` dataset.\n\nData Preparation\n----------------\n\n`MINC <http://opensurfaces.cs.cornell.edu/publications/minc/>`__ is\nshort for Materials in Context Database, provided by Cornell.\n``MINC-2500`` is a resized subset of ``MINC`` with 23 classes, and 2500\nimages in each class. It is well labeled and has a moderate size thus is\nperfect to be our example.\n\n|image-minc|\n\nTo start, we first download ``MINC-2500`` from\n`here <http://opensurfaces.cs.cornell.edu/publications/minc/>`__.\nSuppose we have the data downloaded to ``~/data/`` and\nextracted to ``~/data/minc-2500``.\n\nAfter extraction, it occupies around 2.6GB disk space with the following\nstructure:\n\n::\n\n    minc-2500\n    \u251c\u2500\u2500 README.txt\n    \u251c\u2500\u2500 categories.txt\n    \u251c\u2500\u2500 images\n    \u2514\u2500\u2500 labels\n\nThe ``images`` folder has 23 sub-folders for 23 classes, and ``labels``\nfolder contains five different splits for training, validation, and test.\n\nWe have written a script to prepare the data for you:\n\n:download:`Download prepare_minc.py<../../../scripts/classification/finetune/prepare_minc.py>`\n\nRun it with\n\n::\n\n    python prepare_minc.py --data ~/data/minc-2500 --split 1\n\nNow we have the following structure:\n\n::\n\n    minc-2500\n    \u251c\u2500\u2500 categories.txt\n    \u251c\u2500\u2500 images\n    \u251c\u2500\u2500 labels\n    \u251c\u2500\u2500 README.txt\n    \u251c\u2500\u2500 test\n    \u251c\u2500\u2500 train\n    \u2514\u2500\u2500 val\n\nIn order to go through this tutorial within a reasonable amount of time,\nwe have prepared a small subset of the ``MINC-2500`` dataset,\nbut you should substitute it with the original dataset for your experiments.\nWe can download and extract it with:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import zipfile, os\nfrom gluoncv.utils import download\n\nfile_url = 'https://raw.githubusercontent.com/dmlc/web-data/master/gluoncv/classification/minc-2500-tiny.zip'\nzip_file = download(file_url, path='./')\nwith zipfile.ZipFile(zip_file, 'r') as zin:\n    zin.extractall(os.path.expanduser('./'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hyperparameters\n----------\n\nFirst, let's import all other necessary libraries.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import mxnet as mx\nimport numpy as np\nimport os, time, shutil\n\nfrom mxnet import gluon, image, init, nd\nfrom mxnet import autograd as ag\nfrom mxnet.gluon import nn\nfrom mxnet.gluon.data.vision import transforms\nfrom gluoncv.utils import makedirs\nfrom gluoncv.model_zoo import get_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We set the hyperparameters as following:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "classes = 23\n\nepochs = 5\nlr = 0.001\nper_device_batch_size = 1\nmomentum = 0.9\nwd = 0.0001\n\nlr_factor = 0.75\nlr_steps = [10, 20, 30, np.inf]\n\nnum_gpus = 1\nnum_workers = 8\nctx = [mx.gpu(i) for i in range(num_gpus)] if num_gpus > 0 else [mx.cpu()]\nbatch_size = per_device_batch_size * max(num_gpus, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Things to keep in mind:\n\n1. ``epochs = 5`` is just for this tutorial with the tiny dataset. please change it to a larger number in your experiments, for instance 40.\n2. ``per_device_batch_size`` is also set to a small number. In your experiments you can try larger number like 64.\n3. remember to tune ``num_gpus`` and ``num_workers`` according to your machine.\n4. A pre-trained model is already in a pretty good status. So we can start with a small ``lr``.\n\nData Augmentation\n-----------------\n\nIn transfer learning, data augmentation can also help.\nWe use the following augmentation in training:\n\n2. Randomly crop the image and resize it to 224x224\n3. Randomly flip the image horizontally\n4. Randomly jitter color and add noise\n5. Transpose the data from height*width*num_channels to num_channels*height*width, and map values from [0, 255] to [0, 1]\n6. Normalize with the mean and standard deviation from the ImageNet dataset.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "jitter_param = 0.4\nlighting_param = 0.1\n\ntransform_train = transforms.Compose([\n    transforms.RandomResizedCrop(224),\n    transforms.RandomFlipLeftRight(),\n    transforms.RandomColorJitter(brightness=jitter_param, contrast=jitter_param,\n                                 saturation=jitter_param),\n    transforms.RandomLighting(lighting_param),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\ntransform_test = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With the data augmentation functions, we can define our data loaders:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "path = './minc-2500-tiny'\ntrain_path = os.path.join(path, 'train')\nval_path = os.path.join(path, 'val')\ntest_path = os.path.join(path, 'test')\n\ntrain_data = gluon.data.DataLoader(\n    gluon.data.vision.ImageFolderDataset(train_path).transform_first(transform_train),\n    batch_size=batch_size, shuffle=True, num_workers=num_workers)\n\nval_data = gluon.data.DataLoader(\n    gluon.data.vision.ImageFolderDataset(val_path).transform_first(transform_test),\n    batch_size=batch_size, shuffle=False, num_workers = num_workers)\n\ntest_data = gluon.data.DataLoader(\n    gluon.data.vision.ImageFolderDataset(test_path).transform_first(transform_test),\n    batch_size=batch_size, shuffle=False, num_workers = num_workers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that only ``train_data`` uses ``transform_train``, while\n``val_data`` and ``test_data`` use ``transform_test`` to produce deterministic\nresults for evaluation.\n\nModel and Trainer\n-----------------\n\nWe use a pre-trained ``ResNet50_v2`` model, which has balanced accuracy and\ncomputation cost.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model_name = 'ResNet50_v2'\nfinetune_net = get_model(model_name, pretrained=True)\nwith finetune_net.name_scope():\n    finetune_net.output = nn.Dense(classes)\nfinetune_net.output.initialize(init.Xavier(), ctx = ctx)\nfinetune_net.collect_params().reset_ctx(ctx)\nfinetune_net.hybridize()\n\ntrainer = gluon.Trainer(finetune_net.collect_params(), 'sgd', {\n                        'learning_rate': lr, 'momentum': momentum, 'wd': wd})\nmetric = mx.metric.Accuracy()\nL = gluon.loss.SoftmaxCrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here's an illustration of the pre-trained model\nand our newly defined model:\n\n|image-model|\n\nSpecifically, we define the new model by::\n\n1. load the pre-trained model\n2. re-define the output layer for the new task\n3. train the network\n\nThis is called \"fine-tuning\", i.e. we have a model trained on another task,\nand we would like to tune it for the dataset we have in hand.\n\nWe define a evaluation function for validation and testing.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def test(net, val_data, ctx):\n    metric = mx.metric.Accuracy()\n    for i, batch in enumerate(val_data):\n        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0, even_split=False)\n        label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0, even_split=False)\n        outputs = [net(X) for X in data]\n        metric.update(label, outputs)\n\n    return metric.get()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training Loop\n-------------\n\nFollowing is the main training loop. It is the same as the loop in\n`CIFAR10 <dive_deep_cifar10.html>`__\nand ImageNet.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>Once again, in order to go through the tutorial faster, we are training on a small\n    subset of the original ``MINC-2500`` dataset, and for only 5 epochs. By training on the\n    full dataset with 40 epochs, it is expected to get accuracy around 80% on test data.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "lr_counter = 0\nnum_batch = len(train_data)\n\nfor epoch in range(epochs):\n    if epoch == lr_steps[lr_counter]:\n        trainer.set_learning_rate(trainer.learning_rate*lr_factor)\n        lr_counter += 1\n\n    tic = time.time()\n    train_loss = 0\n    metric.reset()\n\n    for i, batch in enumerate(train_data):\n        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0, even_split=False)\n        label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0, even_split=False)\n        with ag.record():\n            outputs = [finetune_net(X) for X in data]\n            loss = [L(yhat, y) for yhat, y in zip(outputs, label)]\n        for l in loss:\n            l.backward()\n\n        trainer.step(batch_size)\n        train_loss += sum([l.mean().asscalar() for l in loss]) / len(loss)\n\n        metric.update(label, outputs)\n\n    _, train_acc = metric.get()\n    train_loss /= num_batch\n\n    _, val_acc = test(finetune_net, val_data, ctx)\n\n    print('[Epoch %d] Train-acc: %.3f, loss: %.3f | Val-acc: %.3f | time: %.1f' %\n             (epoch, train_acc, train_loss, val_acc, time.time() - tic))\n\n_, test_acc = test(finetune_net, test_data, ctx)\nprint('[Finished] Test-acc: %.3f' % (test_acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next\n----\n\nNow that you have learned to muster the power of transfer\nlearning, to learn more about training a model on\nImageNet, please read `this tutorial <dive_deep_imagenet.html>`__.\n\nThe idea of transfer learning is the basis of\n`object detection <../examples_detection/index.html>`_ and\n`semantic segmentation <../examples_segmentation/index.html>`_,\nthe next two chapters of our tutorial.\n\n.. |image-minc| image:: https://raw.githubusercontent.com/dmlc/web-data/master/gluoncv/datasets/MINC-2500.png\n.. |image-model| image:: https://zh.gluon.ai/_images/fine-tuning.svg\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}